
# Outline

* Regression model building
* Regression model testing
* Regression diagnostics
* Transformation
* Reporting results of regression
* Multiple regression
* Repeated measures data




# Data formatting

* Wide format
* Long form



## Wide format 

* The 1st column is the concentration of aluminum (AL) that sugar maple seeds were treated with.  
* Each ROW is a different tree
* Height growth was then measured for 4 weeks
* Height was recored in a seperate column
* This is called "wide" format b/c data for an individual thing being studied is read left to right
* This is a common way to collect data and present it in a table and is easy to read



The size of the dataframe

dat.orig <- read.csv(file = "data_orig.csv")
dim(dat.orig)


#### Data in wide format

head(dat.orig)




## Long format

* This is how R needs data to be formatted for regression and ANOVA
* t.test also uses data in this format
* Note that there are MANY rows of data
* Data in this format is not very easy to read by eye
* This format matches how the math gets done by the computer
* ALL respone data (y variables, "height") are in a SINGLE column
* ALL predictor data (x variable, week) are in a single columns

The size of the dataframe

dat.long <- read.csv(file = "data_long.csv")
dim(data.long)



#### Data in long format



head(data.long)




# Plotting regression data

* Plot regression style data wit the plot() command


plot(height ~ week, 
     data = data.long,
     main = "Seedling growth: Height ~ week"
     )


### Extra: changing plotting symbols and other features

_You can change ..._
* color using col = 2 (or another number)
* symbol with pch = ...
* x axis label w/ xlab = 
* y axis label w/ ylab = 


# Basic regression analysis 1: height ~ week

* We are going to do a basic regression analysis of these data.  
* A VERY important caveat here is that we are ignoring the fact that the same seedlings were measured multiple times  
* We are therefore treating each row of data as independent * That is, as if EACH ROW represented a completley different plant that had not been measured before and will not be measured again!
* This is therefore a completely invalid analysis and just done for illustration!
* In reality, this data is considered "repeated measures" or "longitudinal"
* Proper analysis requires calculating averages growth for each plant, using just the final measurement, or using special statistical techniques

## Model fitting

We fill fit a "null" and alternative model.

### Fit a null and alternative model

"Null"" Model

model.null <- lm(height ~ 1, 
     data = data.long)



"Alternative"" Model

model.alt <- lm(height ~ week, 
     data = data.long)


## Test the models


anova(model.null,
      model.alt)


The F statistic and p-value are very important quantities that need to be reported in a paper.

**TASK:** Write the F-statistc and p-value for test on the worksheet.

**NOTE:** Because we sampled the sample plant multiple tiems this p-value is WAY too small and our sample size is WAY too big.  This is a form of psuedoreplication.


## Examine the model against the raw data


par(mfrow = c(1,2))
plot(height ~ week, data = data.long,
     main = "Height ~ 1"
     )
abline(model.null, col = 2, lwd = 3)

plot(height ~ week, 
     data = data.long,
     main = "Height ~ week"
     )
abline(model.alt, col = 2, lwd = 3)




### Extra: 

_Add annotations:_
_You can add annotations using the "text" command.
Try running this 
text(y = 300, x = 0, "sample text",pos = 4)_


## Examine the model parameters

* The parameters tell us the slope and intercept
* We can get them with the summary() command along with lots of other info
* Parameters are also sometimes called "coefficients"
* The R command coef() will give you just the slope and itnercept w/o any other info

Use the summary command to get teh parameters


summary(model.alt)


**TASK:** 
* Write the model equation on the worksheet
* First write it in words: "height ~ intercept + slope*week".  
* Then write it with the numbers (properly rounded) included



## R^2

* R^2 (pronounced "r-squared") tells us how much information our model contains
* A high R^2 means that our model captures a lot of the variablity in the data
* A low R^2 means the model isn't telling us much
* If we fit a regression to data that all falls in a straight line, R^2 will equal 1.0

**TASK:** 
* Write down the "adjusted R^2"
* Is the model telling us much?



## Regression diagnostics

### Get model residuals

Use the resid() command

my.resid <- resid(model.alt)


### Model diagnostic: Histogram of residual

This gives insight into whether the assumption of normality is met


hist(my.resid)



### Model diagnosic: qqplot

* qqplots are a better way to look at normality.  
* "qq" stands for "Quantile-quantile".  
* We want the dots in the plot to fall close to the dotted line.

par(mfrow = c(1,1))
plot(model.alt, 2)



### Get model predictions

Also called "Fitted values".  Mathematicall they are called "y.hat"


my.fitted <- fitted(model.alt)


Fitted values (aka predictions) are obtained by plugging in your observations (x values) into the model and calculating the y value.



### Diagnostic: Residual vs Fitted

This tests the assumption of "constant variance"


plot(my.resid ~ my.fitted)
abline(h = 0, col  = 2, lwd = 2)


The fanning shape of the residual is a very bad sign that the assumtpon of constant variance has been violated.




## Transformations

We can fix some problems with normality and non-cosntant variance via transformation.

### Refit model to transformed data

* Note use of log() within the model formula
* In R, "log()" gives your the "natural log", not the base-10 log!


model.alt.log <- lm( log(height) ~ week, data = data.long)




### Look at diagnostics of transformed data


Make a histogram of the residuals

my.resid.log <- resid(model.alt.log)
hist(my.resid.log)



More normal!



qqplot

plot(model.alt.log, 2)



Points fall closer to the line!

### Compare origianl and transformed data


par(mfrow = c(1,2), mar = c(1,1,4,1))
plot(model.alt, 2, main = "origingal")
plot(model.alt.log, 2, main = "log trans")




Plot residuals vs. fitted (predicted values)


par(mfrow = c(1,1))
my.fitted.log <- fitted(model.alt.log)
plot(my.resid.log ~ my.fitted.log)
abline(h = 0, col  = 2, lwd = 2)


Not so fan-shapped any more



# Reporting results of regression

* Results get reported in words, tables and figures in science
* There is some redundancy at times between them (things get repeatd), but that's ok

## What to report for regression in the text of your paper

* the F-statistic 
* the degrees of freedoom of the F-test
* the p-value for the F-statistic
* the slope of the line
* the standard error (SE) of the line
* The R^2

This is a lot of stuff, but all of it is important!  NOte that the slope can be considered a type of "efffect size" (more specifically, an un-standardized effect size).  The standard error of the slope tells us how precisely we have estimated.

## Reporting the result of these analyses

I can get all this info I need summary(model.alt.log)

I would write these results like this:
"There was a strong relationship (F = ____, df = 1,266, p = ____ the height of sugar maple seedlings and time (slope = ____, SE = _____, R^2 = _____)"

**TASK:** On the worksheet complete this sentence with the ppropirate info.

## Why R^2 is important to report!

These results look pretty impressive when we read them.  The p-value is small, the slope is positive, etc.  However, when we get to the R^2 value, what do we learn?  The model, even though its p-value is very small, is not really telling us much of why the data is so variable.  We're explaining one aspect of the variability in the data (variation over time) but 95% of the variation [ (1 - 0.05)*100 ] remains unexplained





# Multiple regression

These data are from an experiment where plants were treated with aluminum, from 0 to 600 (grams?).  We can see if there is an impact of the AL on growth


plot(height ~ conc.AL, data = data.long)


This plot isn't very helpful because the process of growth over time is the main biological process.  The effect of AL is to _change_ the growth rate.  

Regression allows us to look at multiple factors at the same time.  We can therefore look at the impact of time on growth and AL on growth.  A  full modeling of this process will take some work; I'll jsut introduce the most basic elemetns now.  

When you include multiple predictor variables (x variables) in a model its called **multiple regression**.  We can do this in R like this:



## A basic multiple linear regression model

model.alt.2 <- lm(height ~ week + conc.AL, 
                    data = data.long)


Note the "+" sign between week and conc.AL

## Testing hypotheses with multiple regression

We can see whether how original "alt" model, with just the week varibble, compares to a multiple linear regression model with both week and AL.  We do this with the anova() command as we have done for comparisons between null and alt models


anova(model.alt,
      model.alt.2)



**TASK:** Write the F-statistc and p-value for test on the worksheet for the multiple regression.

## Formula for multiple linear reression

Get the parameters using the summary() command

summary(model.alt.2)


There are three lines for the parameters.  We now have one intercepts and TWO DIFFERNT slope values.

**TASK:** Write the word formula and mathematical formula for the multiple regression model.

The word formula is:
height ~ intercept + slope1*conc.AL + slope2*week



### R^2 from multiple regression

Adding more predictor variables allows us to explain more of what is going on in a dataset.  R^2 will ALWAYS go down when we add a new variable


summary(model.alt.2)



**TASK:** Write R^2 for the multiple regression model.




# Endpoint analysis of "Repeated measures" data

* All of the analyses so far are not appropriate for these data.  
* This is is because multiple measurements have been made on the same plant, violating the assumption of independence central to standard statistics
* This type of data is common 
* When you have this kind of data, a good way to start your analysis is by just looking at the last time point
* We can consider this an "end point analysis" of our "longitudinal data"

Let's go back to the original "wide" dataset


head(dat.orig)


* The last column has the height of the seedlings at the end of the 4 week experiment.  
* We can do a regression with these data using the concentration of aluminum



## End point analysis: Plot height ~ AL for just week 4


plot(ht.wk.4 ~ conc.AL, 
                 data = dat.orig)



## End point analysis: Model height ~ AL for week four

Build the model

model.wk.4 <- lm(ht.wk.4 ~ conc.AL, 
                 data = dat.orig)



## Compare end-point model to data

** TASK:** Make a graph of the model plotted on the data.
This requires the plot() and abline() command.  The code was used above.


## Look at the results of end-point model

Look at the results

summary(model.wk.4)


** TASK ** Write a sentence that summarizes this model



## Look at model diagnostics for end-point analysis

We can get the residuals vs. fitted diagnostic plot in a single step below using "plot(model.wk.4, which = 1)".  We get the qqplot with "which = 2".


par(mfrow = c(1,2))
plot(model.wk.4, which = 1) #residuals vs. fitted
plot(model.wk.4, which = 2) #qqplot



** TASK:** Print off this diagnostic plot and write several sentences about what it tells you.



## Transform end-point model

A log transformation might improve the model.  Implement this and check the residuals.  Note that you can transform the data within the model funciton as "log(ht.wk.4)".  

** TASK:** Write the code needed for the transformation onto the worksheet.

